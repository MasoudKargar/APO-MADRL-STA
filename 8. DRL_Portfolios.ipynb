{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv3IDvrobU37"
   },
   "source": [
    "# Asset Portfolio Management using Deep Reinforcement Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kHCfEiTA80V"
   },
   "source": [
    "## 8.0 Deep Reinforcement Learning Portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "matplotlib.use('Agg')\n",
    "import datetime\n",
    "from pypfopt import expected_returns, risk_models, EfficientFrontier, objective_functions\n",
    "from config import config\n",
    "from backtest import backtest_strat, baseline_strat\n",
    "import env_portfolio\n",
    "from env_portfolio import StockPortfolioEnv\n",
    "import models\n",
    "from models import DRLAgent\n",
    "from finrl.preprocessing.data import data_split\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "\n",
    "%store -r train_df\n",
    "%store -r test_df\n",
    "\n",
    "best_return=True\n",
    "\n",
    "while(best_return):\n",
    "    tech_indicator_list = ['f01','f02','f03','f04']\n",
    "    stock_dimension = len(train_df.tic.unique())\n",
    "    state_space = stock_dimension\n",
    "    print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "    weights_initial = [1/stock_dimension]*stock_dimension\n",
    "    env_kwargs = {\n",
    "        \"hmax\": 500, \n",
    "        \"initial_amount\": 1000000, \n",
    "        \"transaction_cost_pct\": 0.001, \n",
    "        \"state_space\": state_space, \n",
    "        \"stock_dim\": stock_dimension, \n",
    "        \"tech_indicator_list\": tech_indicator_list, \n",
    "        \"action_space\": stock_dimension, \n",
    "        \"reward_scaling\": 0,\n",
    "        'initial_weights': [1/stock_dimension]*stock_dimension\n",
    "    }\n",
    "    e_train_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_train, _ = e_train_gym.get_sb_env()\n",
    "    print(type(env_train))\n",
    "    # initialize\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    A2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.001}\n",
    "    model_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)\n",
    "    trained_a2c = agent.train_model(model=model_a2c,    tb_log_name='a2c',   total_timesteps=10000)\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    PPO_PARAMS = {\n",
    "        \"n_steps\": 5,\n",
    "        \"ent_coef\": 0.005,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 100,\n",
    "    }\n",
    "    model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "    trained_ppo = agent.train_model(model=model_ppo,   tb_log_name='ppo',  total_timesteps=10000)\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    DDPG_PARAMS = {\"batch_size\": 100, \"buffer_size\": 50000, \"learning_rate\": 0.001}\n",
    "\n",
    "\n",
    "    model_ddpg = agent.get_model(\"ddpg\",model_kwargs = DDPG_PARAMS)\n",
    "\n",
    "    trained_ddpg = agent.train_model(model=model_ddpg,  tb_log_name='ddpg', total_timesteps=10000)\n",
    "\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    TD3_PARAMS = {\"batch_size\": 100,\n",
    "                \"buffer_size\": 50000,\n",
    "                \"learning_rate\": 0.001}\n",
    "\n",
    "    model_td3 = agent.get_model(\"td3\", model_kwargs=TD3_PARAMS)\n",
    "\n",
    "    trained_td3 = agent.train_model(\n",
    "        model=model_td3, tb_log_name='td3', total_timesteps=10000)\n",
    "\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    SAC_PARAMS = {\n",
    "        \"batch_size\": 100,\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"learning_starts\": 100,\n",
    "        \"ent_coef\": \"auto_0.1\",\n",
    "    }\n",
    "\n",
    "    model_sac = agent.get_model(\"sac\", model_kwargs=SAC_PARAMS)\n",
    "\n",
    "    trained_sac = agent.train_model(\n",
    "        model=model_sac, tb_log_name='sac', total_timesteps=10000)\n",
    "\n",
    "    # A2C Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    a2c_train_daily_return, a2c_train_weights = DRLAgent.DRL_prediction(model=trained_a2c, test_data = train_df, test_env = env_trade, test_obs = obs_trade)\n",
    "\n",
    "    # PPO Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    ppo_train_daily_return, ppo_train_weights = DRLAgent.DRL_prediction(model=trained_ppo, test_data = train_df,test_env = env_trade,test_obs = obs_trade)\n",
    "\n",
    "    # DDPG Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    ddpg_train_daily_return, ddpg_train_weights = DRLAgent.DRL_prediction(model=trained_ddpg, test_data = train_df,  test_env = env_trade, test_obs = obs_trade)\n",
    "\n",
    "    # td3 Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    td3_train_daily_return, td3_train_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_td3, test_data=train_df,  test_env=env_trade, test_obs=obs_trade)\n",
    "\n",
    "    # sac Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    sac_train_daily_return, sac_train_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_sac, test_data=train_df,  test_env=env_trade, test_obs=obs_trade)\n",
    "\n",
    "    # Store the Training Models\n",
    "    %store a2c_train_daily_return\n",
    "    %store ppo_train_daily_return\n",
    "    %store ddpg_train_daily_return\n",
    "    %store td3_train_daily_return\n",
    "    %store sac_train_daily_return\n",
    "\n",
    "    %store a2c_train_weights\n",
    "    %store ppo_train_weights\n",
    "    %store ddpg_train_weights\n",
    "    %store td3_train_weights\n",
    "    %store sac_train_weights\n",
    "\n",
    "    a2c_train_daily_return.to_csv('a2c_train_daily_return.csv',index=False)\n",
    "    ppo_train_daily_return.to_csv('ppo_train_daily_return.csv',index=False)\n",
    "    ddpg_train_daily_return.to_csv('ddpg_train_daily_return.csv',index=False)\n",
    "    td3_train_daily_return.to_csv('td3_train_daily_return.csv',index=False)\n",
    "    sac_train_daily_return.to_csv('sac_train_daily_return.csv',index=False)\n",
    "    a2c_train_weights.to_csv('a2c_train_weights.csv',index=False)\n",
    "    ppo_train_weights.to_csv('ppo_train_weights.csv',index=False)\n",
    "    ddpg_train_weights.to_csv('ddpg_train_weights.csv',index=False)\n",
    "    td3_train_weights.to_csv('td3_train_weights.csv',index=False)\n",
    "    sac_train_weights.to_csv('sac_train_weights.csv',index=False)\n",
    "\n",
    "    # A2C Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    a2c_test_daily_return, a2c_test_weights = DRLAgent.DRL_prediction(model=trained_a2c, test_data = test_df, test_env = env_trade, test_obs = obs_trade)\n",
    "\n",
    "    a2c_test_weights.to_csv('a2c_test_weights.csv')\n",
    "    a2c_test_daily_return.to_csv('a2c_test_daily_return.csv')\n",
    "\n",
    "    # PPO Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    ppo_test_daily_return, ppo_test_weights = DRLAgent.DRL_prediction(model=trained_ppo, test_data = test_df, test_env = env_trade, test_obs = obs_trade)\n",
    "\n",
    "    ppo_test_weights.to_csv('ppo_test_weights.csv')\n",
    "    ppo_test_daily_return.to_csv('ppo_test_daily_return.csv')\n",
    "\n",
    "    # DDPG Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    ddpg_test_daily_return, ddpg_test_weights = DRLAgent.DRL_prediction(model=trained_ddpg, test_data = test_df, test_env = env_trade,test_obs = obs_trade)\n",
    "\n",
    "    ddpg_test_weights.to_csv('ddpg_test_weights.csv')\n",
    "    ddpg_test_daily_return.to_csv('ddpg_test_daily_return.csv')\n",
    "\n",
    "    # td3 Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    td3_test_daily_return, td3_test_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_td3, test_data=test_df, test_env=env_trade, test_obs=obs_trade)\n",
    "\n",
    "    td3_test_weights.to_csv('td3_test_weights.csv')\n",
    "    td3_test_daily_return.to_csv('td3_test_daily_return.csv')\n",
    "\n",
    "    # sac Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    sac_test_daily_return, sac_test_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_sac, test_data=test_df, test_env=env_trade, test_obs=obs_trade)\n",
    "\n",
    "\n",
    "    sac_test_weights.to_csv('sac_test_weights.csv')\n",
    "    sac_test_daily_return.to_csv('sac_test_daily_return.csv')\n",
    "\n",
    "    a2c_test_portfolio = a2c_test_weights.copy()\n",
    "    a2c_test_returns = a2c_test_daily_return.copy()\n",
    "\n",
    "    ppo_test_portfolio = ppo_test_weights.copy()\n",
    "    ppo_test_returns = ppo_test_daily_return.copy()\n",
    "\n",
    "    ddpg_test_portfolio = ddpg_test_weights.copy()\n",
    "    ddpg_test_returns = ddpg_test_daily_return.copy()\n",
    "\n",
    "    td3_test_portfolio = td3_test_weights.copy()\n",
    "    td3_test_returns = td3_test_daily_return.copy()\n",
    "\n",
    "    sac_test_portfolio = sac_test_weights.copy()\n",
    "    sac_test_returns = sac_test_daily_return.copy()\n",
    "\n",
    "    %store a2c_test_portfolio\n",
    "    %store a2c_test_returns \n",
    "    %store ppo_test_portfolio\n",
    "    %store ppo_test_returns \n",
    "    %store ddpg_test_portfolio\n",
    "    %store ddpg_test_returns\n",
    "    %store td3_test_portfolio\n",
    "    %store td3_test_returns\n",
    "    %store sac_test_portfolio\n",
    "    %store sac_test_returns\n",
    "\n",
    "    # First proposed Method\n",
    "\n",
    "    a2c_test_weights2 = pd.read_csv('a2c_test_weights.csv')\n",
    "    ppo_test_weights2 = pd.read_csv('ppo_test_weights.csv')\n",
    "    ddpg_test_weights2 = pd.read_csv('ddpg_test_weights.csv')\n",
    "    td3_test_weights2 = pd.read_csv('td3_test_weights.csv')\n",
    "    sac_test_weights2 = pd.read_csv('sac_test_weights.csv')\n",
    "    a2c_test_weights_dropdate = a2c_test_weights2.drop(columns=['date'])\n",
    "    ppo_test_weights_dropdate = ppo_test_weights2.drop(columns=['date'])\n",
    "    ddpg_test_weights_dropdate = ddpg_test_weights2.drop(columns=['date'])\n",
    "    td3_test_weights_dropdate = td3_test_weights2.drop(columns=['date'])\n",
    "    sac_test_weights_dropdate = sac_test_weights2.drop(columns=['date'])\n",
    "\n",
    "    all_test_weights = a2c_test_weights_dropdate + ppo_test_weights_dropdate + \\\n",
    "        ddpg_test_weights_dropdate + td3_test_weights_dropdate + sac_test_weights_dropdate\n",
    "\n",
    "    all_agents_normalized_test_weights_avg = all_test_weights/5\n",
    "    all_agents_normalized_test_weights_avg.to_csv('all_agents_normalized_test_weights_avg.csv', index=False)\n",
    "\n",
    "    %store all_agents_normalized_test_weights_avg\n",
    "    %store -r df_close_full_stocks\n",
    "    %store -r filtered_stocks\n",
    "\n",
    "    start_date = '2021-03-29'\n",
    "    end_date = '2024-03-28'\n",
    "    filtered_df = df_close_full_stocks[(df_close_full_stocks['date'] < end_date) & (df_close_full_stocks['date'] >= start_date)]\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "    columns_to_drop = [col for col in filtered_df if col not in filtered_stocks]\n",
    "    df_kept = filtered_df.drop(columns=columns_to_drop)\n",
    "    df1 = df_kept.reindex(sorted(df_kept.columns), axis=1)\n",
    "    test_close_normal = df1.pct_change()\n",
    "\n",
    "\n",
    "    shahin_test_weights = pd.read_csv('all_agents_normalized_test_weights_avg.csv')\n",
    "    final = test_close_normal * shahin_test_weights\n",
    "    final.to_csv('all_agents_normalized_eachstock_return.csv', index=False)\n",
    "\n",
    "    row_sums = []\n",
    "\n",
    "    for index, row in final.iterrows():\n",
    "        row_sum = row.sum()\n",
    "        row_sums.append(row_sum)\n",
    "\n",
    "    all_agents_normalized_test_daily_return = pd.DataFrame({'daily_return': row_sums})\n",
    "    a2c_test_weights = pd.read_csv('a2c_test_weights.csv')\n",
    "    all_agents_normalized_test_daily_return.insert(1, 'date', a2c_test_weights['date'])\n",
    "    all_agents_normalized_test_daily_return = all_agents_normalized_test_daily_return[['date','daily_return']]\n",
    "    all_agents_normalized_test_daily_return.to_csv('all_agents_normalized_test_daily_return.csv', index=False)\n",
    "    %store all_agents_normalized_test_daily_return\n",
    "\n",
    "    test_close_normal.to_csv('mydata/testclosenormalpct.csv')\n",
    "\n",
    "    %store -r a2c_train_daily_return\n",
    "    %store -r ppo_train_daily_return\n",
    "    %store -r ddpg_train_daily_return\n",
    "    %store -r td3_train_daily_return\n",
    "    %store -r sac_train_daily_return\n",
    "\n",
    "\n",
    "    #Main proposed Method\n",
    "\n",
    "    days = 10\n",
    "\n",
    "    a2c_test_daily_return = pd.read_csv('a2c_test_daily_return.csv')\n",
    "    ppo_test_daily_return = pd.read_csv('ppo_test_daily_return.csv')\n",
    "    ddpg_test_daily_return = pd.read_csv('ddpg_test_daily_return.csv')\n",
    "    td3_test_daily_return = pd.read_csv('td3_test_daily_return.csv')\n",
    "    sac_test_daily_return = pd.read_csv('sac_test_daily_return.csv')\n",
    "    a2c_test_daily_return = a2c_test_daily_return.drop(columns=['date'])\n",
    "    ppo_test_daily_return = ppo_test_daily_return.drop(columns=['date'])\n",
    "    ddpg_test_daily_return = ddpg_test_daily_return.drop(columns=['date'])\n",
    "    td3_test_daily_return = td3_test_daily_return.drop(columns=['date'])\n",
    "    sac_test_daily_return = sac_test_daily_return.drop(columns=['date'])\n",
    "    a2c_test_daily_return = a2c_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    ppo_test_daily_return = ppo_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    ddpg_test_daily_return = ddpg_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    td3_test_daily_return = td3_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    sac_test_daily_return = sac_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    dfs = [a2c_test_daily_return, ppo_test_daily_return, ddpg_test_daily_return, td3_test_daily_return,\n",
    "        sac_test_daily_return]\n",
    "\n",
    "    merged_test_daily_return = pd.concat(dfs, axis=1)\n",
    "    merged_test_daily_return.columns = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    merged_test_daily_return.to_csv('merged_test_daily_return.csv')\n",
    "\n",
    "\n",
    "    columns_to_sum = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['a2c'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,\n",
    "                                    'sumavg_10_a2c'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['ppo'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,\n",
    "                                    'sumavg_10_ppo'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['ddpg'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,\n",
    "                                    'sumavg_10_ddpg'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['td3'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,\n",
    "                                    'sumavg_10_td3'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['sac'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,\n",
    "                                    'sumavg_10_sac'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "\n",
    "    merged_test_daily_return.to_csv('test_sum_avg.csv')\n",
    "\n",
    "    merged_test_daily_return_y = merged_test_daily_return.iloc[days:, 5:10]\n",
    "    merged_test_daily_return_y.reset_index(drop=True, inplace=True)\n",
    "    my_test_Y = merged_test_daily_return_y\n",
    "    my_test_Y.to_csv('my_test_Y.csv')\n",
    "\n",
    "    def assign_max(row):\n",
    "        max_value = row.max()  # Find the maximum value in the row\n",
    "        if max_value == 0:\n",
    "            return [1] + [0] * (len(row) - 1)\n",
    "        else:\n",
    "            return [1 if val == max_value else 0 for val in row]\n",
    "\n",
    "\n",
    "    max_df = my_test_Y.apply(assign_max, axis=1, result_type='expand')\n",
    "    max_df.columns = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    max_df.to_csv('my_test_Y_zero_one.csv')\n",
    "    max_df\n",
    "\n",
    "    hosieni_test_weights = pd.read_csv('my_test_Y_zero_one.csv', header=0)\n",
    "    hosieni_test_weights = hosieni_test_weights.drop(columns=['Unnamed: 0'])\n",
    "    zeros_df = pd.DataFrame(\n",
    "        np.zeros((days, hosieni_test_weights.shape[1])), columns=hosieni_test_weights.columns)\n",
    "    zeros_df['td3'] = 0\n",
    "    hosieni_test_weights = pd.concat(\n",
    "        [zeros_df, hosieni_test_weights], ignore_index=True)\n",
    "\n",
    "    max_df = hosieni_test_weights\n",
    "    max_df.to_csv('max_df.csv',index=False)\n",
    "\n",
    "    a2c_test_weights2 = pd.read_csv('a2c_test_weights.csv')\n",
    "    ppo_test_weights2 = pd.read_csv('ppo_test_weights.csv')\n",
    "    ddpg_test_weights2 = pd.read_csv('ddpg_test_weights.csv')\n",
    "    td3_test_weights2 = pd.read_csv('td3_test_weights.csv')\n",
    "    sac_test_weights2 = pd.read_csv('sac_test_weights.csv')\n",
    "    a2c_test_weights = a2c_test_weights2.drop(columns=['date'])\n",
    "    ppo_test_weights = ppo_test_weights2.drop(columns=['date'])\n",
    "    ddpg_test_weights = ddpg_test_weights2.drop(columns=['date'])\n",
    "    td3_test_weights = td3_test_weights2.drop(columns=['date'])\n",
    "    sac_test_weights = sac_test_weights2.drop(columns=['date'])\n",
    "    a2c_test_weights.reset_index(drop=True, inplace=True)\n",
    "    ppo_test_weights.reset_index(drop=True, inplace=True)\n",
    "    ddpg_test_weights.reset_index(drop=True, inplace=True)\n",
    "    td3_test_weights.reset_index(drop=True, inplace=True)\n",
    "    sac_test_weights.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    a2c_merged_df = pd.concat([a2c_test_weights, max_df.iloc[:, 0]], axis=1)\n",
    "    ppo_merged_df = pd.concat([ppo_test_weights, max_df.iloc[:, 1]], axis=1)\n",
    "    ddpg_merged_df = pd.concat([ddpg_test_weights, max_df.iloc[:, 2]], axis=1)\n",
    "    td3_merged_df = pd.concat([td3_test_weights, max_df.iloc[:, 3]], axis=1)\n",
    "    sac_merged_df = pd.concat([sac_test_weights, max_df.iloc[:, 4]], axis=1)\n",
    "\n",
    "    td3_merged_df\n",
    "    a2c_merged_df.iloc[:, 0:]\n",
    "\n",
    "    a2c_merged_df.iloc[:, :-1] *= a2c_merged_df.iloc[:, -1].values[:, None]\n",
    "    ppo_merged_df.iloc[:, :-1] *= ppo_merged_df.iloc[:, -1].values[:, None]\n",
    "    ddpg_merged_df.iloc[:, :-1] *= ddpg_merged_df.iloc[:, -1].values[:, None]\n",
    "    td3_merged_df.iloc[:, :-1] *= td3_merged_df.iloc[:, -1].values[:, None]\n",
    "    sac_merged_df.iloc[:, :-1] *= sac_merged_df.iloc[:, -1].values[:, None]\n",
    "    a2c_merged_df = a2c_merged_df.drop(columns=['a2c'])\n",
    "    ppo_merged_df = ppo_merged_df.drop(columns=['ppo'])\n",
    "    ddpg_merged_df = ddpg_merged_df.drop(columns=['ddpg'])\n",
    "    td3_merged_df = td3_merged_df.drop(columns=['td3'])\n",
    "    sac_merged_df = sac_merged_df.drop(columns=['sac'])\n",
    "\n",
    "    dfs = [a2c_merged_df, ppo_merged_df, ddpg_merged_df, td3_merged_df, sac_merged_df]\n",
    "    cols_to_sum = dfs[0].columns[0:]\n",
    "    above_sum_df6 = pd.DataFrame(columns=cols_to_sum)\n",
    "    for df in dfs:\n",
    "        above_sum_df6 = above_sum_df6.add(df[cols_to_sum], fill_value=0)\n",
    "\n",
    "    above_sum_df6.to_csv('weghits_hosieni.csv')\n",
    "\n",
    "    %store -r df_close_full_stocks\n",
    "    %store -r filtered_stocks\n",
    "\n",
    "    start_date = '2021-03-29'\n",
    "    end_date = '2024-03-28'\n",
    "    filtered_df = df_close_full_stocks[(df_close_full_stocks['date'] < end_date) & (df_close_full_stocks['date'] >= start_date)]\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "    columns_to_drop = [col for col in filtered_df if col not in filtered_stocks]\n",
    "    df_kept = filtered_df.drop(columns=columns_to_drop)\n",
    "    df1 = df_kept.reindex(sorted(df_kept.columns), axis=1)\n",
    "    test_close_normal = df1.pct_change()\n",
    "\n",
    "    hoseini_algorithm = above_sum_df6\n",
    "    hosini_daily_return = test_close_normal * hoseini_algorithm\n",
    "    hosini_daily_return.to_csv('hosini_daily_return.csv', index=False)\n",
    "\n",
    "    daily_returns = df1.pct_change().fillna(0)\n",
    "    daily_returns = daily_returns.replace([float('inf'), float('-inf')], 0)\n",
    "    portfolio_returns = (daily_returns * above_sum_df6).sum(axis=1)\n",
    "    cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "\n",
    "    a2c_test_cum_returns = (1 + a2c_test_returns['daily_return']).cumprod()\n",
    "    ppo_test_cum_returns = (1 + ppo_test_returns['daily_return']).cumprod()\n",
    "    ddpg_test_cum_returns = (1 + ddpg_test_returns['daily_return']).cumprod()\n",
    "    td3_test_cum_returns = (1 + td3_test_returns['daily_return']).cumprod()\n",
    "    sac_test_cum_returns = (1 + sac_test_returns['daily_return']).cumprod()\n",
    "\n",
    "    maximum = max(a2c_test_cum_returns.iloc[-1], ppo_test_cum_returns.iloc[-1], ddpg_test_cum_returns.iloc[-1], td3_test_cum_returns.iloc[-1], sac_test_cum_returns.iloc[-1])\n",
    "    if (cumulative_returns.iloc[-1]>maximum):\n",
    "        best_return=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_list = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "for days in days_list:\n",
    "    a2c_test_daily_return = pd.read_csv('a2c_test_daily_return.csv')\n",
    "    ppo_test_daily_return = pd.read_csv('ppo_test_daily_return.csv')\n",
    "    ddpg_test_daily_return = pd.read_csv('ddpg_test_daily_return.csv')\n",
    "    td3_test_daily_return = pd.read_csv('td3_test_daily_return.csv')\n",
    "    sac_test_daily_return = pd.read_csv('sac_test_daily_return.csv')\n",
    "    a2c_test_daily_return = a2c_test_daily_return.drop(columns=['date'])\n",
    "    ppo_test_daily_return = ppo_test_daily_return.drop(columns=['date'])\n",
    "    ddpg_test_daily_return = ddpg_test_daily_return.drop(columns=['date'])\n",
    "    td3_test_daily_return = td3_test_daily_return.drop(columns=['date'])\n",
    "    sac_test_daily_return = sac_test_daily_return.drop(columns=['date'])\n",
    "    a2c_test_daily_return = a2c_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    ppo_test_daily_return = ppo_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    ddpg_test_daily_return = ddpg_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    td3_test_daily_return = td3_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    sac_test_daily_return = sac_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    dfs = [a2c_test_daily_return, ppo_test_daily_return, ddpg_test_daily_return, td3_test_daily_return,\n",
    "        sac_test_daily_return]\n",
    "\n",
    "    merged_test_daily_return = pd.concat(dfs, axis=1)\n",
    "    merged_test_daily_return.columns = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    merged_test_daily_return.to_csv('merged_test_daily_return.csv')\n",
    "\n",
    "\n",
    "    columns_to_sum = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['a2c'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,'sumavg_10_a2c'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['ppo'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,'sumavg_10_ppo'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "        \n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['ddpg'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,'sumavg_10_ddpg'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['td3'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,'sumavg_10_td3'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['sac'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,'sumavg_10_sac'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "\n",
    "    merged_test_daily_return.to_csv('test_sum_avg.csv')\n",
    "\n",
    "    merged_test_daily_return_y = merged_test_daily_return.iloc[days:, 5:10]\n",
    "    merged_test_daily_return_y.reset_index(drop=True, inplace=True)\n",
    "    my_test_Y = merged_test_daily_return_y\n",
    "    my_test_Y.to_csv('my_test_Y.csv')\n",
    "\n",
    "    def assign_max(row):\n",
    "        max_value = row.max()  # Find the maximum value in the row\n",
    "        if max_value == 0:\n",
    "            return [1] + [0] * (len(row) - 1)\n",
    "        else:\n",
    "            return [1 if val == max_value else 0 for val in row]\n",
    "\n",
    "\n",
    "    max_df = my_test_Y.apply(assign_max, axis=1, result_type='expand')\n",
    "    max_df.columns = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    max_df.to_csv('my_test_Y_zero_one.csv')\n",
    "    max_df\n",
    "\n",
    "    hosieni_test_weights = pd.read_csv('my_test_Y_zero_one.csv', header=0)\n",
    "    hosieni_test_weights = hosieni_test_weights.drop(columns=['Unnamed: 0'])\n",
    "    zeros_df = pd.DataFrame(np.zeros((days, hosieni_test_weights.shape[1])), columns=hosieni_test_weights.columns)\n",
    "    zeros_df['td3'] = 0\n",
    "    hosieni_test_weights = pd.concat(\n",
    "        [zeros_df, hosieni_test_weights], ignore_index=True)\n",
    "\n",
    "    max_df = hosieni_test_weights\n",
    "    max_df.to_csv('max_df.csv',index=False)\n",
    "\n",
    "    a2c_test_weights2 = pd.read_csv('a2c_test_weights.csv')\n",
    "    ppo_test_weights2 = pd.read_csv('ppo_test_weights.csv')\n",
    "    ddpg_test_weights2 = pd.read_csv('ddpg_test_weights.csv')\n",
    "    td3_test_weights2 = pd.read_csv('td3_test_weights.csv')\n",
    "    sac_test_weights2 = pd.read_csv('sac_test_weights.csv')\n",
    "    a2c_test_weights = a2c_test_weights2.drop(columns=['date'])\n",
    "    ppo_test_weights = ppo_test_weights2.drop(columns=['date'])\n",
    "    ddpg_test_weights = ddpg_test_weights2.drop(columns=['date'])\n",
    "    td3_test_weights = td3_test_weights2.drop(columns=['date'])\n",
    "    sac_test_weights = sac_test_weights2.drop(columns=['date'])\n",
    "    a2c_test_weights.reset_index(drop=True, inplace=True)\n",
    "    ppo_test_weights.reset_index(drop=True, inplace=True)\n",
    "    ddpg_test_weights.reset_index(drop=True, inplace=True)\n",
    "    td3_test_weights.reset_index(drop=True, inplace=True)\n",
    "    sac_test_weights.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    a2c_merged_df = pd.concat([a2c_test_weights, max_df.iloc[:, 0]], axis=1)\n",
    "    ppo_merged_df = pd.concat([ppo_test_weights, max_df.iloc[:, 1]], axis=1)\n",
    "    ddpg_merged_df = pd.concat([ddpg_test_weights, max_df.iloc[:, 2]], axis=1)\n",
    "    td3_merged_df = pd.concat([td3_test_weights, max_df.iloc[:, 3]], axis=1)\n",
    "    sac_merged_df = pd.concat([sac_test_weights, max_df.iloc[:, 4]], axis=1)\n",
    "\n",
    "    td3_merged_df\n",
    "    a2c_merged_df.iloc[:, 0:]\n",
    "\n",
    "    a2c_merged_df.iloc[:, :-1] *= a2c_merged_df.iloc[:, -1].values[:, None]\n",
    "    ppo_merged_df.iloc[:, :-1] *= ppo_merged_df.iloc[:, -1].values[:, None]\n",
    "    ddpg_merged_df.iloc[:, :-1] *= ddpg_merged_df.iloc[:, -1].values[:, None]\n",
    "    td3_merged_df.iloc[:, :-1] *= td3_merged_df.iloc[:, -1].values[:, None]\n",
    "    sac_merged_df.iloc[:, :-1] *= sac_merged_df.iloc[:, -1].values[:, None]\n",
    "    a2c_merged_df = a2c_merged_df.drop(columns=['a2c'])\n",
    "    ppo_merged_df = ppo_merged_df.drop(columns=['ppo'])\n",
    "    ddpg_merged_df = ddpg_merged_df.drop(columns=['ddpg'])\n",
    "    td3_merged_df = td3_merged_df.drop(columns=['td3'])\n",
    "    sac_merged_df = sac_merged_df.drop(columns=['sac'])\n",
    "\n",
    "    dfs = [a2c_merged_df, ppo_merged_df, ddpg_merged_df, td3_merged_df, sac_merged_df]\n",
    "    cols_to_sum = dfs[0].columns[0:]\n",
    "    above_sum_df6 = pd.DataFrame(columns=cols_to_sum)\n",
    "    for df in dfs:\n",
    "        above_sum_df6 = above_sum_df6.add(df[cols_to_sum], fill_value=0)\n",
    "\n",
    "    above_sum_df6.to_csv('weghits_hosieni.csv')\n",
    "\n",
    "    %store -r df_close_full_stocks\n",
    "    %store -r filtered_stocks\n",
    "\n",
    "    start_date = '2021-03-29'\n",
    "    end_date = '2024-03-28'\n",
    "    filtered_df = df_close_full_stocks[(df_close_full_stocks['date'] < end_date) & (df_close_full_stocks['date'] >= start_date)]\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "    columns_to_drop = [col for col in filtered_df if col not in filtered_stocks]\n",
    "    df_kept = filtered_df.drop(columns=columns_to_drop)\n",
    "    df1 = df_kept.reindex(sorted(df_kept.columns), axis=1)\n",
    "    test_close_normal = df1.pct_change()\n",
    "\n",
    "    hoseini_algorithm = above_sum_df6\n",
    "    hosini_daily_return = test_close_normal * hoseini_algorithm\n",
    "    hosini_daily_return.to_csv('hosini_daily_return.csv', index=False)\n",
    "\n",
    "    daily_returns = df1.pct_change().fillna(0)\n",
    "    daily_returns = daily_returns.replace([float('inf'), float('-inf')], 0)\n",
    "    portfolio_returns = (daily_returns * above_sum_df6).sum(axis=1)\n",
    "    cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "\n",
    "    print(f\"Days: {days}, Cumulative Return: {cumulative_returns.iloc[-1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lvrqTro3lhAh",
    "a3Iuv554xYFH",
    "SPEXBcm-uBJo"
   ],
   "include_colab_link": true,
   "name": "FinRL_portfolio_allocation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
