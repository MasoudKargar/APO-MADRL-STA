{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv3IDvrobU37"
   },
   "source": [
    "# Asset Portfolio Management using Deep Reinforcement Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kHCfEiTA80V"
   },
   "source": [
    "## 8.0 Deep Reinforcement Learning Portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 20, State Space: 20\n",
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_347\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -28      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 1.59e+08 |\n",
      "|    std                | 0.983    |\n",
      "|    value_loss         | 3.38e+13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 362       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -28       |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 2.12e+08  |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 5.97e+13  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 353      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -28      |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 2.7e+08  |\n",
      "|    std                | 0.98     |\n",
      "|    value_loss         | 1.04e+14 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 346      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27.9    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 3.23e+08 |\n",
      "|    std                | 0.977    |\n",
      "|    value_loss         | 1.55e+14 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 345      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 3.9e+08  |\n",
      "|    std                | 0.97     |\n",
      "|    value_loss         | 2.73e+14 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 339      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 6e+08    |\n",
      "|    std                | 0.969    |\n",
      "|    value_loss         | 4.9e+14  |\n",
      "------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:7100592.442223951\n",
      "Sharpe:  1.0671084985297707\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 337      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27.7    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.34e+08 |\n",
      "|    std                | 0.967    |\n",
      "|    value_loss         | 3.3e+13  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 342      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.85e+08 |\n",
      "|    std                | 0.966    |\n",
      "|    value_loss         | 5.83e+13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 346       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -27.6     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 2.85e+08  |\n",
      "|    std                | 0.962     |\n",
      "|    value_loss         | 1.07e+14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 347      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27.5    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 2.93e+08 |\n",
      "|    std                | 0.96     |\n",
      "|    value_loss         | 1.33e+14 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 350      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 3.95e+08 |\n",
      "|    std                | 0.957    |\n",
      "|    value_loss         | 2.34e+14 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 352       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -27.4     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 5.29e+08  |\n",
      "|    std                | 0.954     |\n",
      "|    value_loss         | 3.95e+14  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6583902.104788125\n",
      "Sharpe:  1.0309554279669308\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 351      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 1.35e+08 |\n",
      "|    std                | 0.953    |\n",
      "|    value_loss         | 3.23e+13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 352       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -27.3     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 1.96e+08  |\n",
      "|    std                | 0.949     |\n",
      "|    value_loss         | 5.26e+13  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 352      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 2.48e+08 |\n",
      "|    std                | 0.945    |\n",
      "|    value_loss         | 1.01e+14 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 352      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 2.73e+08 |\n",
      "|    std                | 0.942    |\n",
      "|    value_loss         | 1.38e+14 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 353       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -27.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 3.92e+08  |\n",
      "|    std                | 0.936     |\n",
      "|    value_loss         | 2.78e+14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 354      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 4.71e+08 |\n",
      "|    std                | 0.933    |\n",
      "|    value_loss         | 4.02e+14 |\n",
      "------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6670375.289469513\n",
      "Sharpe:  1.0447960936325047\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 351       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -26.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 1.42e+08  |\n",
      "|    std                | 0.929     |\n",
      "|    value_loss         | 2.99e+13  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 350      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -26.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.001    |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 1.72e+08 |\n",
      "|    std                | 0.925    |\n",
      "|    value_loss         | 5.13e+13 |\n",
      "------------------------------------\n",
      "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.001, 'batch_size': 100}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_368\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 526  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5992492.776475727\n",
      "Sharpe:  0.998041713653965\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 458       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 8         |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -28.4     |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 7.89e+14  |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | -2.94e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.7e+15   |\n",
      "---------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:7021307.179459047\n",
      "Sharpe:  1.0736259422173424\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 447       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 13        |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -28.4     |\n",
      "|    explained_variance   | 2.38e-07  |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 1.86e+15  |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -2.56e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.37e+15  |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 437      |\n",
      "|    iterations           | 4        |\n",
      "|    time_elapsed         | 18       |\n",
      "|    total_timesteps      | 8192     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -28.4    |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.001    |\n",
      "|    loss                 | 2.35e+15 |\n",
      "|    n_updates            | 30       |\n",
      "|    policy_gradient_loss | -8.6e-07 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 5.1e+15  |\n",
      "--------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6507512.057679593\n",
      "Sharpe:  1.0338628594537436\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 435       |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 23        |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -28.4     |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 1.04e+15  |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -1.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.93e+15  |\n",
      "---------------------------------------\n",
      "{'batch_size': 100, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_331\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6558565.197095844\n",
      "Sharpe:  1.0344603006255138\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6884376.670797464\n",
      "Sharpe:  1.0537877421937663\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6884376.670797464\n",
      "Sharpe:  1.0537877421937663\n",
      "=================================\n",
      "{'batch_size': 100, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3\\td3_273\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6570170.946591378\n",
      "Sharpe:  1.0232148707405628\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6591877.143878575\n",
      "Sharpe:  1.0224553355564256\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6591877.143878575\n",
      "Sharpe:  1.0224553355564256\n",
      "=================================\n",
      "{'batch_size': 100, 'buffer_size': 50000, 'learning_rate': 0.001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac\\sac_268\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6013036.828980888\n",
      "Sharpe:  1.020603399831242\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5946992.488715032\n",
      "Sharpe:  1.0201923282160783\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5947047.551319049\n",
      "Sharpe:  1.0201918362012539\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6892771.751114328\n",
      "Sharpe:  1.0587762805916063\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\nice\\anaconda3\\envs\\rein\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6438292.368394902\n",
      "Sharpe:  1.0251404146370457\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\nice\\anaconda3\\envs\\rein\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6884376.670797464\n",
      "Sharpe:  1.0537877421937663\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\nice\\anaconda3\\envs\\rein\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6591877.143878575\n",
      "Sharpe:  1.0224553355564256\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\nice\\anaconda3\\envs\\rein\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5948871.522437984\n",
      "Sharpe:  1.020339394750235\n",
      "=================================\n",
      "Stored 'a2c_train_daily_return' (DataFrame)\n",
      "Stored 'ppo_train_daily_return' (DataFrame)\n",
      "Stored 'ddpg_train_daily_return' (DataFrame)\n",
      "Stored 'td3_train_daily_return' (DataFrame)\n",
      "Stored 'sac_train_daily_return' (DataFrame)\n",
      "Stored 'a2c_train_weights' (DataFrame)\n",
      "Stored 'ppo_train_weights' (DataFrame)\n",
      "Stored 'ddpg_train_weights' (DataFrame)\n",
      "Stored 'td3_train_weights' (DataFrame)\n",
      "Stored 'sac_train_weights' (DataFrame)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\nice\\anaconda3\\envs\\rein\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:1321081.8217530018\n",
      "Sharpe:  0.7510567755510236\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\nice\\anaconda3\\envs\\rein\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:1334854.5237146982\n",
      "Sharpe:  0.7735546378703233\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\nice\\anaconda3\\envs\\rein\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:1268776.1715355846\n",
      "Sharpe:  0.6492896322476647\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\nice\\anaconda3\\envs\\rein\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:1351480.0817205487\n",
      "Sharpe:  0.7854485175733013\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\nice\\anaconda3\\envs\\rein\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:1329809.1408749332\n",
      "Sharpe:  0.803504635766319\n",
      "=================================\n",
      "Stored 'a2c_test_portfolio' (DataFrame)\n",
      "Stored 'a2c_test_returns' (DataFrame)\n",
      "Stored 'ppo_test_portfolio' (DataFrame)\n",
      "Stored 'ppo_test_returns' (DataFrame)\n",
      "Stored 'ddpg_test_portfolio' (DataFrame)\n",
      "Stored 'ddpg_test_returns' (DataFrame)\n",
      "Stored 'td3_test_portfolio' (DataFrame)\n",
      "Stored 'td3_test_returns' (DataFrame)\n",
      "Stored 'sac_test_portfolio' (DataFrame)\n",
      "Stored 'sac_test_returns' (DataFrame)\n",
      "Stored 'all_agents_normalized_test_weights_avg' (DataFrame)\n",
      "Stored 'all_agents_normalized_test_daily_return' (DataFrame)\n",
      "1.3210818217530018 1.3348545237147023 1.2687761715355883 1.3514800817205468 1.3298091408749386 1.386438156859907\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "matplotlib.use('Agg')\n",
    "import datetime\n",
    "from pypfopt import expected_returns, risk_models, EfficientFrontier, objective_functions\n",
    "from config import config\n",
    "from backtest import backtest_strat, baseline_strat\n",
    "import env_portfolio\n",
    "from env_portfolio import StockPortfolioEnv\n",
    "import models\n",
    "from models import DRLAgent\n",
    "from finrl.preprocessing.data import data_split\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "\n",
    "%store -r train_df\n",
    "%store -r test_df\n",
    "\n",
    "best_return=True\n",
    "\n",
    "while(best_return):\n",
    "    tech_indicator_list = ['f01','f02','f03','f04']\n",
    "    stock_dimension = len(train_df.tic.unique())\n",
    "    state_space = stock_dimension\n",
    "    print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "    weights_initial = [1/stock_dimension]*stock_dimension\n",
    "    env_kwargs = {\n",
    "        \"hmax\": 500, \n",
    "        \"initial_amount\": 1000000, \n",
    "        \"transaction_cost_pct\": 0.001, \n",
    "        \"state_space\": state_space, \n",
    "        \"stock_dim\": stock_dimension, \n",
    "        \"tech_indicator_list\": tech_indicator_list, \n",
    "        \"action_space\": stock_dimension, \n",
    "        \"reward_scaling\": 0,\n",
    "        'initial_weights': [1/stock_dimension]*stock_dimension\n",
    "    }\n",
    "    e_train_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_train, _ = e_train_gym.get_sb_env()\n",
    "    print(type(env_train))\n",
    "    # initialize\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    A2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.001}\n",
    "    model_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)\n",
    "    trained_a2c = agent.train_model(model=model_a2c,    tb_log_name='a2c',   total_timesteps=10000)\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    PPO_PARAMS = {\n",
    "        \"n_steps\": 5,\n",
    "        \"ent_coef\": 0.005,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 100,\n",
    "    }\n",
    "    model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "    trained_ppo = agent.train_model(model=model_ppo,   tb_log_name='ppo',  total_timesteps=10000)\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    DDPG_PARAMS = {\"batch_size\": 100, \"buffer_size\": 50000, \"learning_rate\": 0.001}\n",
    "\n",
    "\n",
    "    model_ddpg = agent.get_model(\"ddpg\",model_kwargs = DDPG_PARAMS)\n",
    "\n",
    "    trained_ddpg = agent.train_model(model=model_ddpg,  tb_log_name='ddpg', total_timesteps=10000)\n",
    "\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    TD3_PARAMS = {\"batch_size\": 100,\n",
    "                \"buffer_size\": 50000,\n",
    "                \"learning_rate\": 0.001}\n",
    "\n",
    "    model_td3 = agent.get_model(\"td3\", model_kwargs=TD3_PARAMS)\n",
    "\n",
    "    trained_td3 = agent.train_model(\n",
    "        model=model_td3, tb_log_name='td3', total_timesteps=10000)\n",
    "\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    SAC_PARAMS = {\n",
    "        \"batch_size\": 100,\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"learning_starts\": 100,\n",
    "        \"ent_coef\": \"auto_0.1\",\n",
    "    }\n",
    "\n",
    "    model_sac = agent.get_model(\"sac\", model_kwargs=SAC_PARAMS)\n",
    "\n",
    "    trained_sac = agent.train_model(\n",
    "        model=model_sac, tb_log_name='sac', total_timesteps=10000)\n",
    "\n",
    "    # A2C Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    a2c_train_daily_return, a2c_train_weights = DRLAgent.DRL_prediction(model=trained_a2c, test_data = train_df, test_env = env_trade, test_obs = obs_trade)\n",
    "\n",
    "    # PPO Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    ppo_train_daily_return, ppo_train_weights = DRLAgent.DRL_prediction(model=trained_ppo, test_data = train_df,test_env = env_trade,test_obs = obs_trade)\n",
    "\n",
    "    # DDPG Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    ddpg_train_daily_return, ddpg_train_weights = DRLAgent.DRL_prediction(model=trained_ddpg, test_data = train_df,  test_env = env_trade, test_obs = obs_trade)\n",
    "\n",
    "    # td3 Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    td3_train_daily_return, td3_train_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_td3, test_data=train_df,  test_env=env_trade, test_obs=obs_trade)\n",
    "\n",
    "    # sac Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    sac_train_daily_return, sac_train_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_sac, test_data=train_df,  test_env=env_trade, test_obs=obs_trade)\n",
    "\n",
    "    # Store the Training Models\n",
    "    %store a2c_train_daily_return\n",
    "    %store ppo_train_daily_return\n",
    "    %store ddpg_train_daily_return\n",
    "    %store td3_train_daily_return\n",
    "    %store sac_train_daily_return\n",
    "\n",
    "    %store a2c_train_weights\n",
    "    %store ppo_train_weights\n",
    "    %store ddpg_train_weights\n",
    "    %store td3_train_weights\n",
    "    %store sac_train_weights\n",
    "\n",
    "    a2c_train_daily_return.to_csv('a2c_train_daily_return.csv',index=False)\n",
    "    ppo_train_daily_return.to_csv('ppo_train_daily_return.csv',index=False)\n",
    "    ddpg_train_daily_return.to_csv('ddpg_train_daily_return.csv',index=False)\n",
    "    td3_train_daily_return.to_csv('td3_train_daily_return.csv',index=False)\n",
    "    sac_train_daily_return.to_csv('sac_train_daily_return.csv',index=False)\n",
    "    a2c_train_weights.to_csv('a2c_train_weights.csv',index=False)\n",
    "    ppo_train_weights.to_csv('ppo_train_weights.csv',index=False)\n",
    "    ddpg_train_weights.to_csv('ddpg_train_weights.csv',index=False)\n",
    "    td3_train_weights.to_csv('td3_train_weights.csv',index=False)\n",
    "    sac_train_weights.to_csv('sac_train_weights.csv',index=False)\n",
    "\n",
    "    # A2C Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    a2c_test_daily_return, a2c_test_weights = DRLAgent.DRL_prediction(model=trained_a2c, test_data = test_df, test_env = env_trade, test_obs = obs_trade)\n",
    "\n",
    "    a2c_test_weights.to_csv('a2c_test_weights.csv')\n",
    "    a2c_test_daily_return.to_csv('a2c_test_daily_return.csv')\n",
    "\n",
    "    # PPO Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    ppo_test_daily_return, ppo_test_weights = DRLAgent.DRL_prediction(model=trained_ppo, test_data = test_df, test_env = env_trade, test_obs = obs_trade)\n",
    "\n",
    "    ppo_test_weights.to_csv('ppo_test_weights.csv')\n",
    "    ppo_test_daily_return.to_csv('ppo_test_daily_return.csv')\n",
    "\n",
    "    # DDPG Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    ddpg_test_daily_return, ddpg_test_weights = DRLAgent.DRL_prediction(model=trained_ddpg, test_data = test_df, test_env = env_trade,test_obs = obs_trade)\n",
    "\n",
    "    ddpg_test_weights.to_csv('ddpg_test_weights.csv')\n",
    "    ddpg_test_daily_return.to_csv('ddpg_test_daily_return.csv')\n",
    "\n",
    "    # td3 Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    td3_test_daily_return, td3_test_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_td3, test_data=test_df, test_env=env_trade, test_obs=obs_trade)\n",
    "\n",
    "    td3_test_weights.to_csv('td3_test_weights.csv')\n",
    "    td3_test_daily_return.to_csv('td3_test_daily_return.csv')\n",
    "\n",
    "    # sac Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "    sac_test_daily_return, sac_test_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_sac, test_data=test_df, test_env=env_trade, test_obs=obs_trade)\n",
    "\n",
    "\n",
    "    sac_test_weights.to_csv('sac_test_weights.csv')\n",
    "    sac_test_daily_return.to_csv('sac_test_daily_return.csv')\n",
    "\n",
    "    a2c_test_portfolio = a2c_test_weights.copy()\n",
    "    a2c_test_returns = a2c_test_daily_return.copy()\n",
    "\n",
    "    ppo_test_portfolio = ppo_test_weights.copy()\n",
    "    ppo_test_returns = ppo_test_daily_return.copy()\n",
    "\n",
    "    ddpg_test_portfolio = ddpg_test_weights.copy()\n",
    "    ddpg_test_returns = ddpg_test_daily_return.copy()\n",
    "\n",
    "    td3_test_portfolio = td3_test_weights.copy()\n",
    "    td3_test_returns = td3_test_daily_return.copy()\n",
    "\n",
    "    sac_test_portfolio = sac_test_weights.copy()\n",
    "    sac_test_returns = sac_test_daily_return.copy()\n",
    "\n",
    "    %store a2c_test_portfolio\n",
    "    %store a2c_test_returns \n",
    "    %store ppo_test_portfolio\n",
    "    %store ppo_test_returns \n",
    "    %store ddpg_test_portfolio\n",
    "    %store ddpg_test_returns\n",
    "    %store td3_test_portfolio\n",
    "    %store td3_test_returns\n",
    "    %store sac_test_portfolio\n",
    "    %store sac_test_returns\n",
    "\n",
    "    # First proposed Method\n",
    "\n",
    "    a2c_test_weights2 = pd.read_csv('a2c_test_weights.csv')\n",
    "    ppo_test_weights2 = pd.read_csv('ppo_test_weights.csv')\n",
    "    ddpg_test_weights2 = pd.read_csv('ddpg_test_weights.csv')\n",
    "    td3_test_weights2 = pd.read_csv('td3_test_weights.csv')\n",
    "    sac_test_weights2 = pd.read_csv('sac_test_weights.csv')\n",
    "    a2c_test_weights_dropdate = a2c_test_weights2.drop(columns=['date'])\n",
    "    ppo_test_weights_dropdate = ppo_test_weights2.drop(columns=['date'])\n",
    "    ddpg_test_weights_dropdate = ddpg_test_weights2.drop(columns=['date'])\n",
    "    td3_test_weights_dropdate = td3_test_weights2.drop(columns=['date'])\n",
    "    sac_test_weights_dropdate = sac_test_weights2.drop(columns=['date'])\n",
    "\n",
    "    all_test_weights = a2c_test_weights_dropdate + ppo_test_weights_dropdate + \\\n",
    "        ddpg_test_weights_dropdate + td3_test_weights_dropdate + sac_test_weights_dropdate\n",
    "\n",
    "    all_agents_normalized_test_weights_avg = all_test_weights/5\n",
    "    all_agents_normalized_test_weights_avg.to_csv('all_agents_normalized_test_weights_avg.csv', index=False)\n",
    "\n",
    "    %store all_agents_normalized_test_weights_avg\n",
    "    %store -r df_close_full_stocks\n",
    "    %store -r filtered_stocks\n",
    "\n",
    "    start_date = '2021-03-29'\n",
    "    end_date = '2024-03-28'\n",
    "    filtered_df = df_close_full_stocks[(df_close_full_stocks['date'] < end_date) & (df_close_full_stocks['date'] >= start_date)]\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "    columns_to_drop = [col for col in filtered_df if col not in filtered_stocks]\n",
    "    df_kept = filtered_df.drop(columns=columns_to_drop)\n",
    "    df1 = df_kept.reindex(sorted(df_kept.columns), axis=1)\n",
    "    test_close_normal = df1.pct_change()\n",
    "\n",
    "\n",
    "    shahin_test_weights = pd.read_csv('all_agents_normalized_test_weights_avg.csv')\n",
    "    final = test_close_normal * shahin_test_weights\n",
    "    final.to_csv('all_agents_normalized_eachstock_return.csv', index=False)\n",
    "\n",
    "    row_sums = []\n",
    "\n",
    "    for index, row in final.iterrows():\n",
    "        row_sum = row.sum()\n",
    "        row_sums.append(row_sum)\n",
    "\n",
    "    all_agents_normalized_test_daily_return = pd.DataFrame({'daily_return': row_sums})\n",
    "    a2c_test_weights = pd.read_csv('a2c_test_weights.csv')\n",
    "    all_agents_normalized_test_daily_return.insert(1, 'date', a2c_test_weights['date'])\n",
    "    all_agents_normalized_test_daily_return = all_agents_normalized_test_daily_return[['date','daily_return']]\n",
    "    all_agents_normalized_test_daily_return.to_csv('all_agents_normalized_test_daily_return.csv', index=False)\n",
    "    %store all_agents_normalized_test_daily_return\n",
    "\n",
    "    test_close_normal.to_csv('mydata/testclosenormalpct.csv')\n",
    "\n",
    "    %store -r a2c_train_daily_return\n",
    "    %store -r ppo_train_daily_return\n",
    "    %store -r ddpg_train_daily_return\n",
    "    %store -r td3_train_daily_return\n",
    "    %store -r sac_train_daily_return\n",
    "\n",
    "\n",
    "    #Main proposed Method\n",
    "\n",
    "    days = 10\n",
    "\n",
    "    a2c_test_daily_return = pd.read_csv('a2c_test_daily_return.csv')\n",
    "    ppo_test_daily_return = pd.read_csv('ppo_test_daily_return.csv')\n",
    "    ddpg_test_daily_return = pd.read_csv('ddpg_test_daily_return.csv')\n",
    "    td3_test_daily_return = pd.read_csv('td3_test_daily_return.csv')\n",
    "    sac_test_daily_return = pd.read_csv('sac_test_daily_return.csv')\n",
    "    a2c_test_daily_return = a2c_test_daily_return.drop(columns=['date'])\n",
    "    ppo_test_daily_return = ppo_test_daily_return.drop(columns=['date'])\n",
    "    ddpg_test_daily_return = ddpg_test_daily_return.drop(columns=['date'])\n",
    "    td3_test_daily_return = td3_test_daily_return.drop(columns=['date'])\n",
    "    sac_test_daily_return = sac_test_daily_return.drop(columns=['date'])\n",
    "    a2c_test_daily_return = a2c_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    ppo_test_daily_return = ppo_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    ddpg_test_daily_return = ddpg_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    td3_test_daily_return = td3_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    sac_test_daily_return = sac_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    dfs = [a2c_test_daily_return, ppo_test_daily_return, ddpg_test_daily_return, td3_test_daily_return,\n",
    "        sac_test_daily_return]\n",
    "\n",
    "    merged_test_daily_return = pd.concat(dfs, axis=1)\n",
    "    merged_test_daily_return.columns = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    merged_test_daily_return.to_csv('merged_test_daily_return.csv')\n",
    "\n",
    "\n",
    "    columns_to_sum = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['a2c'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,\n",
    "                                    'sumavg_10_a2c'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['ppo'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,\n",
    "                                    'sumavg_10_ppo'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['ddpg'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,\n",
    "                                    'sumavg_10_ddpg'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['td3'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,\n",
    "                                    'sumavg_10_td3'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['sac'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,\n",
    "                                    'sumavg_10_sac'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "\n",
    "    merged_test_daily_return.to_csv('test_sum_avg.csv')\n",
    "\n",
    "    merged_test_daily_return_y = merged_test_daily_return.iloc[days:, 5:10]\n",
    "    merged_test_daily_return_y.reset_index(drop=True, inplace=True)\n",
    "    my_test_Y = merged_test_daily_return_y\n",
    "    my_test_Y.to_csv('my_test_Y.csv')\n",
    "\n",
    "    def assign_max(row):\n",
    "        max_value = row.max()  # Find the maximum value in the row\n",
    "        if max_value == 0:\n",
    "            return [1] + [0] * (len(row) - 1)\n",
    "        else:\n",
    "            return [1 if val == max_value else 0 for val in row]\n",
    "\n",
    "\n",
    "    max_df = my_test_Y.apply(assign_max, axis=1, result_type='expand')\n",
    "    max_df.columns = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    max_df.to_csv('my_test_Y_zero_one.csv')\n",
    "    max_df\n",
    "\n",
    "    hosieni_test_weights = pd.read_csv('my_test_Y_zero_one.csv', header=0)\n",
    "    hosieni_test_weights = hosieni_test_weights.drop(columns=['Unnamed: 0'])\n",
    "    zeros_df = pd.DataFrame(\n",
    "        np.zeros((days, hosieni_test_weights.shape[1])), columns=hosieni_test_weights.columns)\n",
    "    zeros_df['td3'] = 0\n",
    "    hosieni_test_weights = pd.concat(\n",
    "        [zeros_df, hosieni_test_weights], ignore_index=True)\n",
    "\n",
    "    max_df = hosieni_test_weights\n",
    "    max_df.to_csv('max_df.csv',index=False)\n",
    "\n",
    "    a2c_test_weights2 = pd.read_csv('a2c_test_weights.csv')\n",
    "    ppo_test_weights2 = pd.read_csv('ppo_test_weights.csv')\n",
    "    ddpg_test_weights2 = pd.read_csv('ddpg_test_weights.csv')\n",
    "    td3_test_weights2 = pd.read_csv('td3_test_weights.csv')\n",
    "    sac_test_weights2 = pd.read_csv('sac_test_weights.csv')\n",
    "    a2c_test_weights = a2c_test_weights2.drop(columns=['date'])\n",
    "    ppo_test_weights = ppo_test_weights2.drop(columns=['date'])\n",
    "    ddpg_test_weights = ddpg_test_weights2.drop(columns=['date'])\n",
    "    td3_test_weights = td3_test_weights2.drop(columns=['date'])\n",
    "    sac_test_weights = sac_test_weights2.drop(columns=['date'])\n",
    "    a2c_test_weights.reset_index(drop=True, inplace=True)\n",
    "    ppo_test_weights.reset_index(drop=True, inplace=True)\n",
    "    ddpg_test_weights.reset_index(drop=True, inplace=True)\n",
    "    td3_test_weights.reset_index(drop=True, inplace=True)\n",
    "    sac_test_weights.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    a2c_merged_df = pd.concat([a2c_test_weights, max_df.iloc[:, 0]], axis=1)\n",
    "    ppo_merged_df = pd.concat([ppo_test_weights, max_df.iloc[:, 1]], axis=1)\n",
    "    ddpg_merged_df = pd.concat([ddpg_test_weights, max_df.iloc[:, 2]], axis=1)\n",
    "    td3_merged_df = pd.concat([td3_test_weights, max_df.iloc[:, 3]], axis=1)\n",
    "    sac_merged_df = pd.concat([sac_test_weights, max_df.iloc[:, 4]], axis=1)\n",
    "\n",
    "    td3_merged_df\n",
    "    a2c_merged_df.iloc[:, 0:]\n",
    "\n",
    "    a2c_merged_df.iloc[:, :-1] *= a2c_merged_df.iloc[:, -1].values[:, None]\n",
    "    ppo_merged_df.iloc[:, :-1] *= ppo_merged_df.iloc[:, -1].values[:, None]\n",
    "    ddpg_merged_df.iloc[:, :-1] *= ddpg_merged_df.iloc[:, -1].values[:, None]\n",
    "    td3_merged_df.iloc[:, :-1] *= td3_merged_df.iloc[:, -1].values[:, None]\n",
    "    sac_merged_df.iloc[:, :-1] *= sac_merged_df.iloc[:, -1].values[:, None]\n",
    "    a2c_merged_df = a2c_merged_df.drop(columns=['a2c'])\n",
    "    ppo_merged_df = ppo_merged_df.drop(columns=['ppo'])\n",
    "    ddpg_merged_df = ddpg_merged_df.drop(columns=['ddpg'])\n",
    "    td3_merged_df = td3_merged_df.drop(columns=['td3'])\n",
    "    sac_merged_df = sac_merged_df.drop(columns=['sac'])\n",
    "\n",
    "    dfs = [a2c_merged_df, ppo_merged_df, ddpg_merged_df, td3_merged_df, sac_merged_df]\n",
    "    cols_to_sum = dfs[0].columns[0:]\n",
    "    above_sum_df6 = pd.DataFrame(columns=cols_to_sum)\n",
    "    for df in dfs:\n",
    "        above_sum_df6 = above_sum_df6.add(df[cols_to_sum], fill_value=0)\n",
    "\n",
    "    above_sum_df6.to_csv('weghits_hosieni.csv')\n",
    "\n",
    "    %store -r df_close_full_stocks\n",
    "    %store -r filtered_stocks\n",
    "\n",
    "    start_date = '2021-03-29'\n",
    "    end_date = '2024-03-28'\n",
    "    filtered_df = df_close_full_stocks[(df_close_full_stocks['date'] < end_date) & (df_close_full_stocks['date'] >= start_date)]\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "    columns_to_drop = [col for col in filtered_df if col not in filtered_stocks]\n",
    "    df_kept = filtered_df.drop(columns=columns_to_drop)\n",
    "    df1 = df_kept.reindex(sorted(df_kept.columns), axis=1)\n",
    "    test_close_normal = df1.pct_change()\n",
    "\n",
    "    hoseini_algorithm = above_sum_df6\n",
    "    hosini_daily_return = test_close_normal * hoseini_algorithm\n",
    "    hosini_daily_return.to_csv('hosini_daily_return.csv', index=False)\n",
    "\n",
    "    daily_returns = df1.pct_change().fillna(0)\n",
    "    daily_returns = daily_returns.replace([float('inf'), float('-inf')], 0)\n",
    "    portfolio_returns = (daily_returns * above_sum_df6).sum(axis=1)\n",
    "    cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "\n",
    "    a2c_test_cum_returns = (1 + a2c_test_returns['daily_return']).cumprod()\n",
    "    ppo_test_cum_returns = (1 + ppo_test_returns['daily_return']).cumprod()\n",
    "    ddpg_test_cum_returns = (1 + ddpg_test_returns['daily_return']).cumprod()\n",
    "    td3_test_cum_returns = (1 + td3_test_returns['daily_return']).cumprod()\n",
    "    sac_test_cum_returns = (1 + sac_test_returns['daily_return']).cumprod()\n",
    "\n",
    "    maximum = max(a2c_test_cum_returns.iloc[-1], ppo_test_cum_returns.iloc[-1], ddpg_test_cum_returns.iloc[-1], td3_test_cum_returns.iloc[-1], sac_test_cum_returns.iloc[-1])\n",
    "    if (cumulative_returns.iloc[-1]>maximum):\n",
    "        best_return=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days: 5, Cumulative Return: 1.3429278683731287\n",
      "Days: 10, Cumulative Return: 1.386438156859907\n",
      "Days: 15, Cumulative Return: 1.3059448404408334\n",
      "Days: 20, Cumulative Return: 1.342508622151904\n",
      "Days: 25, Cumulative Return: 1.3176355389343553\n",
      "Days: 30, Cumulative Return: 1.2733003814940584\n"
     ]
    }
   ],
   "source": [
    "days_list = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "for days in days_list:\n",
    "    a2c_test_daily_return = pd.read_csv('a2c_test_daily_return.csv')\n",
    "    ppo_test_daily_return = pd.read_csv('ppo_test_daily_return.csv')\n",
    "    ddpg_test_daily_return = pd.read_csv('ddpg_test_daily_return.csv')\n",
    "    td3_test_daily_return = pd.read_csv('td3_test_daily_return.csv')\n",
    "    sac_test_daily_return = pd.read_csv('sac_test_daily_return.csv')\n",
    "    a2c_test_daily_return = a2c_test_daily_return.drop(columns=['date'])\n",
    "    ppo_test_daily_return = ppo_test_daily_return.drop(columns=['date'])\n",
    "    ddpg_test_daily_return = ddpg_test_daily_return.drop(columns=['date'])\n",
    "    td3_test_daily_return = td3_test_daily_return.drop(columns=['date'])\n",
    "    sac_test_daily_return = sac_test_daily_return.drop(columns=['date'])\n",
    "    a2c_test_daily_return = a2c_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    ppo_test_daily_return = ppo_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    ddpg_test_daily_return = ddpg_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    td3_test_daily_return = td3_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "    sac_test_daily_return = sac_test_daily_return.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    dfs = [a2c_test_daily_return, ppo_test_daily_return, ddpg_test_daily_return, td3_test_daily_return,\n",
    "        sac_test_daily_return]\n",
    "\n",
    "    merged_test_daily_return = pd.concat(dfs, axis=1)\n",
    "    merged_test_daily_return.columns = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    merged_test_daily_return.to_csv('merged_test_daily_return.csv')\n",
    "\n",
    "\n",
    "    columns_to_sum = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['a2c'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,'sumavg_10_a2c'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['ppo'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,'sumavg_10_ppo'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "        \n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['ddpg'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,'sumavg_10_ddpg'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['td3'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,'sumavg_10_td3'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "    for i in range(0, len(merged_test_daily_return)):\n",
    "        sum_10_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum(\n",
    "        ).sum()\n",
    "        sum_10_rows = merged_test_daily_return['sac'].iloc[i:i+days].sum()\n",
    "        merged_test_daily_return.loc[i+days:i+days+1,'sumavg_10_sac'] = (sum_10_rows/sum_10_rows_allcolumns)\n",
    "\n",
    "\n",
    "    merged_test_daily_return.to_csv('test_sum_avg.csv')\n",
    "\n",
    "    merged_test_daily_return_y = merged_test_daily_return.iloc[days:, 5:10]\n",
    "    merged_test_daily_return_y.reset_index(drop=True, inplace=True)\n",
    "    my_test_Y = merged_test_daily_return_y\n",
    "    my_test_Y.to_csv('my_test_Y.csv')\n",
    "\n",
    "    def assign_max(row):\n",
    "        max_value = row.max()  # Find the maximum value in the row\n",
    "        if max_value == 0:\n",
    "            return [1] + [0] * (len(row) - 1)\n",
    "        else:\n",
    "            return [1 if val == max_value else 0 for val in row]\n",
    "\n",
    "\n",
    "    max_df = my_test_Y.apply(assign_max, axis=1, result_type='expand')\n",
    "    max_df.columns = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "    max_df.to_csv('my_test_Y_zero_one.csv')\n",
    "    max_df\n",
    "\n",
    "    hosieni_test_weights = pd.read_csv('my_test_Y_zero_one.csv', header=0)\n",
    "    hosieni_test_weights = hosieni_test_weights.drop(columns=['Unnamed: 0'])\n",
    "    zeros_df = pd.DataFrame(np.zeros((days, hosieni_test_weights.shape[1])), columns=hosieni_test_weights.columns)\n",
    "    zeros_df['td3'] = 0\n",
    "    hosieni_test_weights = pd.concat(\n",
    "        [zeros_df, hosieni_test_weights], ignore_index=True)\n",
    "\n",
    "    max_df = hosieni_test_weights\n",
    "    max_df.to_csv('max_df.csv',index=False)\n",
    "\n",
    "    a2c_test_weights2 = pd.read_csv('a2c_test_weights.csv')\n",
    "    ppo_test_weights2 = pd.read_csv('ppo_test_weights.csv')\n",
    "    ddpg_test_weights2 = pd.read_csv('ddpg_test_weights.csv')\n",
    "    td3_test_weights2 = pd.read_csv('td3_test_weights.csv')\n",
    "    sac_test_weights2 = pd.read_csv('sac_test_weights.csv')\n",
    "    a2c_test_weights = a2c_test_weights2.drop(columns=['date'])\n",
    "    ppo_test_weights = ppo_test_weights2.drop(columns=['date'])\n",
    "    ddpg_test_weights = ddpg_test_weights2.drop(columns=['date'])\n",
    "    td3_test_weights = td3_test_weights2.drop(columns=['date'])\n",
    "    sac_test_weights = sac_test_weights2.drop(columns=['date'])\n",
    "    a2c_test_weights.reset_index(drop=True, inplace=True)\n",
    "    ppo_test_weights.reset_index(drop=True, inplace=True)\n",
    "    ddpg_test_weights.reset_index(drop=True, inplace=True)\n",
    "    td3_test_weights.reset_index(drop=True, inplace=True)\n",
    "    sac_test_weights.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    a2c_merged_df = pd.concat([a2c_test_weights, max_df.iloc[:, 0]], axis=1)\n",
    "    ppo_merged_df = pd.concat([ppo_test_weights, max_df.iloc[:, 1]], axis=1)\n",
    "    ddpg_merged_df = pd.concat([ddpg_test_weights, max_df.iloc[:, 2]], axis=1)\n",
    "    td3_merged_df = pd.concat([td3_test_weights, max_df.iloc[:, 3]], axis=1)\n",
    "    sac_merged_df = pd.concat([sac_test_weights, max_df.iloc[:, 4]], axis=1)\n",
    "\n",
    "    td3_merged_df\n",
    "    a2c_merged_df.iloc[:, 0:]\n",
    "\n",
    "    a2c_merged_df.iloc[:, :-1] *= a2c_merged_df.iloc[:, -1].values[:, None]\n",
    "    ppo_merged_df.iloc[:, :-1] *= ppo_merged_df.iloc[:, -1].values[:, None]\n",
    "    ddpg_merged_df.iloc[:, :-1] *= ddpg_merged_df.iloc[:, -1].values[:, None]\n",
    "    td3_merged_df.iloc[:, :-1] *= td3_merged_df.iloc[:, -1].values[:, None]\n",
    "    sac_merged_df.iloc[:, :-1] *= sac_merged_df.iloc[:, -1].values[:, None]\n",
    "    a2c_merged_df = a2c_merged_df.drop(columns=['a2c'])\n",
    "    ppo_merged_df = ppo_merged_df.drop(columns=['ppo'])\n",
    "    ddpg_merged_df = ddpg_merged_df.drop(columns=['ddpg'])\n",
    "    td3_merged_df = td3_merged_df.drop(columns=['td3'])\n",
    "    sac_merged_df = sac_merged_df.drop(columns=['sac'])\n",
    "\n",
    "    dfs = [a2c_merged_df, ppo_merged_df, ddpg_merged_df, td3_merged_df, sac_merged_df]\n",
    "    cols_to_sum = dfs[0].columns[0:]\n",
    "    above_sum_df6 = pd.DataFrame(columns=cols_to_sum)\n",
    "    for df in dfs:\n",
    "        above_sum_df6 = above_sum_df6.add(df[cols_to_sum], fill_value=0)\n",
    "\n",
    "    above_sum_df6.to_csv('weghits_hosieni.csv')\n",
    "\n",
    "    %store -r df_close_full_stocks\n",
    "    %store -r filtered_stocks\n",
    "\n",
    "    start_date = '2021-03-29'\n",
    "    end_date = '2024-03-28'\n",
    "    filtered_df = df_close_full_stocks[(df_close_full_stocks['date'] < end_date) & (df_close_full_stocks['date'] >= start_date)]\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "    columns_to_drop = [col for col in filtered_df if col not in filtered_stocks]\n",
    "    df_kept = filtered_df.drop(columns=columns_to_drop)\n",
    "    df1 = df_kept.reindex(sorted(df_kept.columns), axis=1)\n",
    "    test_close_normal = df1.pct_change()\n",
    "\n",
    "    hoseini_algorithm = above_sum_df6\n",
    "    hosini_daily_return = test_close_normal * hoseini_algorithm\n",
    "    hosini_daily_return.to_csv('hosini_daily_return.csv', index=False)\n",
    "\n",
    "    daily_returns = df1.pct_change().fillna(0)\n",
    "    daily_returns = daily_returns.replace([float('inf'), float('-inf')], 0)\n",
    "    portfolio_returns = (daily_returns * above_sum_df6).sum(axis=1)\n",
    "    cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "\n",
    "    print(f\"Days: {days}, Cumulative Return: {cumulative_returns.iloc[-1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lvrqTro3lhAh",
    "a3Iuv554xYFH",
    "SPEXBcm-uBJo"
   ],
   "include_colab_link": true,
   "name": "FinRL_portfolio_allocation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
